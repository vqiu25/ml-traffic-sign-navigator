{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "incomplete-ecuador",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "conscious-malawi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8223 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "d = dict()\n",
    "class_labels = {\n",
    "    0: '55',\n",
    "    1: 'Green',\n",
    "    2: 'Line',\n",
    "    3: 'Red',\n",
    "    4: 'Sheep',\n",
    "    5: 'Stop'\n",
    "}\n",
    "\n",
    "# Define the path to your dataset\n",
    "path = '/workspace/jetbot/notebooks/model/dataset'\n",
    "\n",
    "# Image dimensions and batch size\n",
    "img_rows, img_cols = 32, 32\n",
    "batch_size = 11776\n",
    "\n",
    "# Setup ImageDataGenerator\n",
    "datagen = ImageDataGenerator()\n",
    "\n",
    "# Generate data from the directories\n",
    "data = datagen.flow_from_directory(\n",
    "    path + '/myData',\n",
    "    target_size=(img_rows, img_cols),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Get the first batch of images and labels\n",
    "images, labels = next(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "infectious-williams",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def center_crop(image, crop_size):\n",
    "    img_height, img_width = image.shape[:2]\n",
    "    crop_height, crop_width = crop_size\n",
    "\n",
    "    # Calculate the center of the image\n",
    "    center_y, center_x = img_height // 2, img_width // 2\n",
    "\n",
    "    # Calculate the cropping box\n",
    "    crop_y1 = max(0, center_y - crop_height // 2)\n",
    "    crop_y2 = min(img_height, center_y + crop_height // 2)\n",
    "    crop_x1 = max(0, center_x - crop_width // 2)\n",
    "    crop_x2 = min(img_width, center_x + crop_width // 2)\n",
    "\n",
    "    # Return the cropped image\n",
    "    return image[crop_y1:crop_y2, crop_x1:crop_x2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "freelance-fight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape   :(8223, 32, 32, 3)\n",
      "Labels shape :(8223, 6)\n"
     ]
    }
   ],
   "source": [
    "# Labels are one hot encoded\n",
    "print(f\"Data Shape   :{images.shape}\\nLabels shape :{labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "developing-interface",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened Shape: (8223, 3072)\n"
     ]
    }
   ],
   "source": [
    "# Flatten the images for SVM\n",
    "images_flattened = images.reshape(images.shape[0], -1)\n",
    "print(f\"Flattened Shape: {images_flattened.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-tonight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise the flattened images using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "images_flattened_scaled = scaler.fit_transform(images_flattened)\n",
    "print(f\"Flattened Scaled Shape: {images_flattened_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "hindu-corporation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels Shape: (8223,)\n"
     ]
    }
   ],
   "source": [
    "# Convert one-hot encoded labels to integer labels\n",
    "labels_int = np.argmax(labels, axis=1)  # Convert one-hot encoded labels to class indices\n",
    "print(f\"Labels Shape: {labels_int.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "placed-intersection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets (70% train, 30% test)\n",
    "images_train, images_test, labels_train, labels_test = train_test_split(images_flattened, labels_int, test_size=0.3, random_state=0)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cooperative-protest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=0.1, class_weight='balanced', degree=2, gamma=0.01, kernel='poly',\n",
       "    probability=True, verbose=2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = svm.SVC(kernel='poly', C = 0.1, degree = 2, gamma = 0.01, verbose = 2, probability = True, class_weight='balanced')\n",
    "model.fit(images_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "impaired-dispatch",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_test_pred = model.predict(images_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "marked-indie",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00000000\n",
      "Precision: 1.00000000\n",
      "Recall: 1.00000000\n",
      "F1 Score: 1.00000000\n"
     ]
    }
   ],
   "source": [
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score\n",
    "\n",
    "test_accuracy = accuracy_score(labels_test, labels_test_pred)\n",
    "test_precision = precision_score(labels_test, labels_test_pred, average='weighted')\n",
    "test_recall = recall_score(labels_test, labels_test_pred, average='weighted')\n",
    "test_f1 = f1_score(labels_test, labels_test_pred, average='weighted')\n",
    "\n",
    "print(f'Accuracy: {test_accuracy:.8f}')\n",
    "print(f'Precision: {test_precision:.8f}')\n",
    "print(f'Recall: {test_recall:.8f}')\n",
    "print(f'F1 Score: {test_f1:.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "julian-residence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and test dataset have been saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model as a joblib file\n",
    "joblib.dump(model, 'model.joblib')\n",
    "\n",
    "# Save the test dataset (both images and labels) as joblib files\n",
    "# joblib.dump(images_test, 'svm_images_test.joblib')\n",
    "# joblib.dump(labels_test, 'svm_labels_test.joblib')\n",
    "\n",
    "print(\"Model and test dataset have been saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
